{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "    <div style=\"float: left; width: 50%;\">\n",
    "       <img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "    <p style=\"margin: 0; padding-top: 22px; text-align:right;\">22.503 · Programación para la ciencia de datos</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">Grado en Ciencia de Datos Aplicada</p>\n",
    "    <p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programación para la ciencia de datos - PEC2\n",
    "============================\n",
    "\n",
    "En este Notebook encontraréis un conjunto de ejercicios que suponen la segunda actividad de evaluación continua (PEC) de la asignatura.\n",
    "\n",
    "Para cada ejercicio, tened en cuenta que: \n",
    "* **Es necesario añadir comentarios** de vuestro código, que expliquen como habéis implementado la solución del problema planteado.\n",
    "* **Es imprescindible** citar las referencias que habéis consultado para realizar la actividad.\n",
    "* Se valorará que el código proporcionado solucione el problema propuesto y también la calidad del código (comentarios, legibilidad, claridad, uso de las estructuras de datos adecuadas, buena nomenclatura de las variables y funciones, seguimiento del PEP8, etc. ).\n",
    "\n",
    "Veréis que cada una de las actividades tiene asociada una puntuación, que indica el peso que tiene esa actividad sobre la nota de la PEC. Adicionalmente, hay una puntuación asociada a aspectos globales de la PEC (*docstring*, modularidad y estilo), que se evaluaran sobre la totalidad del código entregado. \n",
    "\n",
    "Además, veréis que todas las actividades tienen una etiqueta, que indica los recursos necesarios para llevarla a término. Hay tres posibles etiquetas: \n",
    "\n",
    "* <span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">SM</span> **Solo materiales**: las herramientas necesarias para realizar la actividad se pueden encontrar en los materiales de la asignatura (consideraremos también los materiales de la asignatura Fundamentos de Programación, así como las lecturas obligatorias de material externo que se indiquen en los notebooks).\n",
    "\n",
    "* <span style=\"font-family: Courier New; background-color: #ffcc5c; color: #000000; padding: 3px; \">EG</span> **Consulta externa guiada**: la actividad puede requerir el uso de herramientas que no se encuentren en los materiales de la asignatura, pero el enunciado contiene indicaciones de dónde o cómo encontrar la información adicional necesaria para resolver la actividad.\n",
    "\n",
    "* <span style=\"font-family: Courier New; background-color: #f2ae72; color: #000000; padding: 3px; \">EI</span> **Consulta externa independiente**: la actividad puede requerir el uso de herramientas que no se encuentren en los materiales de la asignatura, y el enunciado puede no incluir la descripción de dónde o cómo encontrar esta información adicional. Será necesario que el estudiante busque esta información usando los recursos explicados en la asignatura.\n",
    "\n",
    "Es importante notar que estas etiquetas no indican el nivel de dificultad del ejercicio, sino únicamente la necesidad de consulta de documentación externa para su resolución. Además, recordad que las **etiquetas son informativas**, pero podréis consultar referencias externas siempre que queráis (aunque no se indique explícitamente) o puede que podáis hacer una actividad sin consultar ningún tipo de documentación. Por ejemplo, para resolver una actividad que solo requiera los materiales de la asignatura, podéis consultar referencias externas si queréis, ¡ya sea tanto para ayudaros en su resolución como para ampliar conocimiento! \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante**: Además de la puntuación de cada ejercicio, que valora la calidad de la solución que proponéis y los resultados obtenidos, en esta PEC se valoran 3 aspectos adicionales, transversales a todas las actividades: \n",
    "\n",
    "* **Documentación** (**0.5*f puntos**): Todas las funciones de los ejercicios de esta PEC deberán estar correctamente documentadas utilizando *docstrings* (en el formato que prefiráis). \n",
    "\n",
    "* **Modularidad** (**1*f puntos**): Se valorará la modularidad del código (la correcta estructuración del código en funciones). \n",
    "\n",
    "* **Estilo** (**0.5*f puntos**): El código tiene que seguir la guía de estilo de Python (PEP8), exceptuando los casos donde hacerlo complique la legibilidad del código.\n",
    "\n",
    "Donde $f$ es la fracción de la PEC realizada. Es decir, la nota de estos aspectos transversales será proporcional a la parte de la PEC realizada.\n",
    "\n",
    "Así, los aspectos transversales tendrán una puntuación máxima de 2 puntos si se entregan todos los ejercicios de la PEC. En caso contrario, la nota de aspectos transversales se multiplicará por la fracción de la PEC entregada. Por ejemplo, si los aspectos transversales están puntuados con un 2 pero solo hay la mitad de la PEC entregada, la nota de aspectos transversales será $2*0.5=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activamos las alertas de estilo\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type hint\n",
    "\n",
    "Os recomendamos resolver esta PEC usando type hints. Podemos entender los type hints de Python como una solución formal para indicar estéticamente el tipo de valor dentro del código Python. Os recomendamos la visualización del siguiente [vídeo](https://www.youtube.com/watch?v=j0dy8Q9VIPk) para una introducción a los type hints en Python, y para entender por qué son interesantes.\n",
    "**El uso de type hints en la resolución de la PEC se bonificará con 0.5 puntos adicionales (para obtener toda la puntuación adicional, hay que implementar los type hints de todas las funciones implementadas en la PEC).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Contestad si son Ciertas o Falsas las siguientes preguntas y razonad brevemente la respuesta:\n",
    "\n",
    "<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">SM</span> **(1 punto)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(a) Tras ejecutar el siguiente código, el valor de la variable x es 1 porque las funciones no pueden modificar las variables globales.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = 1       # var global\n",
    "\n",
    "\n",
    "def threefold(x: int) -> int:\n",
    "    \"\"\"Retorna el valor de x multiplicado por 3\"\"\"\n",
    "    x = x * 3\n",
    "    return x\n",
    "\n",
    "\n",
    "# guardado del resultado de la función en la variable global x\n",
    "x = threefold(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** `Falso` debido a que nuestro valor de **x** es **3** ya a que el resultado de nuestra función se está guardando en la variable global **x**, mas no por que la actualización de dicha variable se la haya realizado mediante (dentro) la función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(b) Definimos la función prod_values(x,y,args) con dos parámetros obligatorios y un número indeterminado de parámetros posicionales de manera que devuelve el producto de todos los parámetros. Si llamamos a la función con los argumentos obligatorios y especificamos un argumento opcional por nombre, prod_values(2,3,z=4) devolverá el valor 24.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError: prod_values() got an unexpected keyword argument 'z'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def prod_values(x: int, y: int, *arg: int) -> int:\n",
    "    r\"\"\"\n",
    "    Retorna el producto de los valores de x, y y el(los) valor(es) de *arg\n",
    "    \"\"\"\n",
    "\n",
    "    return x * y * np.prod(arg)\n",
    "\n",
    "\n",
    "try:\n",
    "    prod_values(2, 3, z=4)\n",
    "except TypeError as e:\n",
    "    print(\"TypeError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** `Falso` ya que se genera un error debido a que con el uso de **`*`** estamos espcificando que nuestros argumentos opcionales serán posicionales, y el argumento **z=4** es un argumento opcional con nombre mas no posicional generandonos así un error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(c) Las funciones pueden devolver otras funciones como valor de retorno.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_funcion(select: str) -> classmethod:\n",
    "    \"\"\"\n",
    "    Select funcion recibe un string que indica la función que deseamos y\n",
    "    retorna la función correspondiente.\n",
    "    \"\"\"\n",
    "    if select == \"suma\":\n",
    "        return sum\n",
    "    elif select == \"media\":\n",
    "        return np.mean\n",
    "    elif select == \"desviacion\":\n",
    "        return np.std\n",
    "    elif select == \"varianza\":\n",
    "        return np.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tipo de la variable de retorno es: <class 'function'>\n",
      "El reultado de la varianza de la lista es: 2.917\n"
     ]
    }
   ],
   "source": [
    "f_deseada = \"varianza\"      # funcion que deseamos aplicar\n",
    "lista = [1, 2, 3, 4, 5, 6]       # lista de valores\n",
    "fun = select_funcion(f_deseada)       # resultado de la funcion guardada en fun\n",
    "\n",
    "# Verificamos el tipo de variable de retorno\n",
    "print(\"El tipo de la variable de retorno es: {}\".format(type(fun)))\n",
    "# aplicamos la función obtenida a la lista de valores\n",
    "print(\"El reultado de la {} de la lista es: {:.3f}\".format(\n",
    "    f_deseada, fun(lista)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** `Verdadero`, tal y como se demuestra en el material didáctico del módulo 2, apartado 1.2.1 y en el ejemplo planteado se donde podemos escribir el nombre completo de nuestra función para posteriormente guradar dicha función de retorno en una variable y aplicarla a una lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(d) La función `map` recibe como parámetros una función y un iterable, y devuelve un iterador que recorre los elementos del iterable en los que la evaluación de la función es True.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# función map para obtener el cuadrado de cada uno de los valores de la lista\n",
    "list(map(lambda x: x**2, lista))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** `Falso` ya que la función map recibe como parámetros una función y un iterable, y **devuelve un iterador que\n",
    "aplica la función proporcionada a cada uno de los elementos del iterable**.\n",
    "El enunciado planteado hace mención a la función **Filter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Una agencia de prensa está elaborando un reportaje sobre la opinión de la población respecto al cambio climático y ha solicitado nuestra colaboración para identificar cuáles son los temas más actuales en este ámbito. Para ayudarles, tenemos que crear una función que extraiga el contenido de los hashtags que comiencen por la palabra \"climate\" (sin tener en cuenta si está escrito en mayúsculas o minúsculas) de una colección de tweets.\n",
    "\n",
    "\n",
    "La función recibirá como *input* la ruta que contiene el archivo con los tweets en formato *.csv*. El *output* será una tupla con el formato `(numero_tweets, lista_hashtags)`, donde \n",
    "- `numero_tweets` indica el número de tweets que contienen hashtags.\n",
    "- `lista_hashtags` es la lista de los hashtags **sin repeticiones**\n",
    "\n",
    "El fichero de entrada que utilizaremos es `twitter_sentiment_data.csv`, que está ubicado en la carpeta `data` y ha sido extraído de https://www.kaggle.com/edqian/twitter-climate-change-sentiment-dataset?select=twitter_sentiment_data.csv.\n",
    "\n",
    "Este fichero está compuesto por un encabezado seguido por un número indeterminado de líneas con el formato\n",
    "```\n",
    "sentiment, message, tweetid\n",
    "```\n",
    "\n",
    "De manera que cada línea del fichero tras el encabezado contiene un tweet.\n",
    "\n",
    "Se incluye, además, una celda extra con el código que debéis utilizar para comprobar que la función se ejecuta correctamente.\n",
    "\n",
    "**Importante:** Utilizad los principios de **programación funcional** que hemos visto en el Notebook de teoría para resolver este ejercicio y **expresiones regulares** para detectar los hashtags.\n",
    "\n",
    "<span style=\"font-family: Courier New; background-color: #ffcc5c; color: #000000; padding: 3px; \">EG</span>  **(2 puntos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36:18: W605 invalid escape sequence '\\w'\n"
     ]
    }
   ],
   "source": [
    "# Respuesta 2\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def hashtags(list_hashtags: list) -> object:\n",
    "    \"\"\" Hastags retorna un objeto el cual los hashtags que coinciden\n",
    "    con #climate\"\"\"\n",
    "    # mediante una regular expretion y método compile su funcion findall,\n",
    "    # buscamos los hashtags que coincidan con #climate de la lista filtrada\n",
    "    return re.compile(r\"#climate\\w+\", re.IGNORECASE).findall(list_hashtags)\n",
    "\n",
    "\n",
    "def find_hashtags(dir: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Find hashtags permite encontrar el número de hashtags que coinciden con\n",
    "    #climate y la lista de los mismos sin repetición.\n",
    "\n",
    "    Argumentos:\n",
    "    dir : str\n",
    "        Ruta del archivo txt que contiene los tweets.\n",
    "\n",
    "    Retorno:\n",
    "    tuple\n",
    "        Tupla con el número de hashtags que coinciden con #climate y la lista\n",
    "        de los mismos sin repetición.\n",
    "    \"\"\"\n",
    "    # con la librería pandas leemos el archivo txt, y seleccionamos la columna\n",
    "    # que contiene los mensajes de los tweets mediante usecols\n",
    "    df = pd.read_csv(dir, usecols=[\"message\"])\n",
    "    # filtramos solo aquellas filas que contienen #climate y otros caracteres\n",
    "    # conc ayuda de una función anónima y regular expretion y sin sin importar\n",
    "    # mayúsculas o minúsculas mediante re.IGNORECASE\n",
    "\n",
    "    filtro_hashtags = list(filter(lambda x: re.compile(\n",
    "        \"#climate\\w+\", re.IGNORECASE).findall(x), df[\"message\"]))\n",
    "    # mediante map enviamos un iterable, en este caso la lista filtrada, a la\n",
    "    # función hashtags para obtener la lista de los palabras que coinciden\n",
    "\n",
    "    lista_hashtags = map(hashtags, filtro_hashtags)\n",
    "    # ya que existen se tiene una lista con listas internas procedemos a\n",
    "    # incluirlas como si fueran una sola lista medinate join y separadas por ,\n",
    "    lista_hashtags = (\",\".join(map(str, lista_hashtags)))\n",
    "\n",
    "    # construimos una tupla con el número de hashtags que coinciden con la\n",
    "    # funcion len y la lista de los mismos sin repetición con ayuda de set,\n",
    "    # mismo que mediante split separamos la lista por cada , y eliminamos\n",
    "    # ciertos caracteres que no nos interesan.\n",
    "    tupla = (len(list(filtro_hashtags)), list(set(re.sub(\n",
    "        r\"\\[|\\]|\\'|#\", \"\", lista_hashtags).split(\",\"))))\n",
    "    return tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(961, ['ClimateActionPH', 'climatefac', 'ClimateJusticeLeague', 'ClimateServices', ' ClimateSummit', 'ClimateJourney18', ' climatechange', 'ClimateACtionNOW', 'climatecounts', 'CLIMATEchange', 'climate_science', 'ClimateJusticeMonth', 'ClimateResilientGrowth', 'ClimateHope', 'ClimateC', ' ClimateMarch', 'ClimateScienceFacts', 'ClimateSA', 'ClimateVoter', 'ClimateVotÃ', 'ClimateNumbskulls', 'ClimateEnergy', 'climateinthetopend', 'climatecha', 'climateadaptation', 'Climategate', 'ClimateChangeHoax', 'ClimateSilence', 'climatechaâ', 'ClimateChageisnormal', 'climatechange2', 'ClimateChangeIs', 'ClimateCÃ', 'ClimateFacts', 'climatefacts', 'ClimateFact', 'climateresolution', 'climatedenial', 'ClimateEmergency', 'ClimateJustice', 'climateontap', 'ClimateGate', ' climateaction', 'ClimateChâ', 'ClimateChange', 'ClimateHustle', 'climatecoin', 'ClimateChanage', 'ClimatePoetry', 'climatejustice', ' ClimateAction', ' climateÃ', 'ClimateIQ', 'climatebarbie', 'climateÃ', 'climatecon', 'ClimateHoax', 'ClimateMarch2017', 'ClimateRisk2015', 'ClimateForum2016', 'climatechang', 'ClimateReality', 'climatechangeisreal', ' climateresilience', 'climatechange', 'ClimateSecurity', 'climatechangeart', 'ClimateCO', 'climatechâ', ' climateâ', 'Climatechange', 'ClimateChangeÃ', 'ClimateMayors', 'ClimateChangeSummerTips', 'ClimateFraudster', 'climateaction', 'ClimateChangeHOAX', ' climatedenial', ' ClimateCrisis', 'climateednyc', ' ClimateHope', 'ClimateChangeWeek', ' ClimateChangâ', 'ClimateImpactsVicâ', 'ClimateScam', 'ClimateEdEntry', ' climatemarch', 'ClimateDeclaration', 'ClimateCountsÃ', 'ClimateCh', 'climateâ', 'climatestepsUS', 'climaterisk15', 'ClimateFraud', 'Climatechangehoax', 'ClimateChangeDenier', 'climatescience', ' ClimateJustice', 'climatetrial', 'ClimateAction', 'ClimateBarbie', 'ClimateForChange', 'ClimateChang', 'climatech', 'climatecents', 'ClimateActÃ', 'ClimateChangeIn5Words', ' ClimateDeniers', 'Climate_change', 'ClimateAlliance', 'climatechangeâ', 'Climate4Impact', 'ClimateChangesHealth', 'ClimateChangeIsReal', 'ClimateTrial', 'climateguide', 'climatechangeAKAweather', 'climateresilience', 'climatemarch', 'ClimateChangeâ', 'climatechangeÃ', 'climatesilence', ' climateactionâ', 'ClimateOptimist', 'climatec', 'ClimatePledgeYVR', 'ClimateStewardship', 'ClimateHour', 'ClimateofHope', 'ClimateAndPeople', 'climatehoax', 'ClimateMarch', ' ClimateChange', 'climatewarrior', 'climatedeniers', 'ClimateScience', 'CLIMATECHANGEâ', 'ClimateCounts', 'ClimateoftheNat', 'climatechangenonsense', 'ClimateAList', 'climatefraud', 'ClimateConference', 'ClimateNPS', 'ClimateÃ', 'ClimateAndEnergy', 'climateCEOs', 'climatesecurity', 'climatechangejobs', 'climateskeptics', ' ClimateChangeIsReal', 'ClimateDepot', 'ClimateWednes', 'ClimateCâ', 'ClimateImpactsVic', 'ClimateCrisis', 'ClimateChan', 'climatechan'])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "hashtags_tuple = find_hashtags(\"data/twitter_sentiment_data.csv\")\n",
    "print(hashtags_tuple)\n",
    "assert hashtags_tuple[0] == 961, \"Not all hashtags found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "La Agencia Estatal de Meteorología va a elaborar su informe trienal sobre la evolución de la contaminación en el período 2018-2020. Para ello, nos han facilitado el archivo `Pollution.csv`, que está ubicado en la carpeta `data` y cuyos datos han sido obtenidos de https://www.aemet.es/es/datos_abiertos/estadisticas/composicion_quimica_atmosfera. Este archivo contiene los registros de concentración de distintas moléculas en microgramos por metro cúbico de aire obtenidos en distintas estaciones meteorológicas. Los datos se registran siempre el último día de cada mes e indican concentraciones mensuales.\n",
    "\n",
    "Nuestra misión consiste en crear dos funciones. En primer lugar, `create_dictionary` que recibirá como *input* el path al fichero de datos, es decir, `data/Pollution.csv` y una fecha en formato string separada por guiones (por ejemplo, \"15-03-2020\"). La función `save_pickle` debe producir un **diccionario** guardado en un **pickle** de manera que para cada molécula se guarden dos listas, una con las fechas de registro consultadas y otra con las concentraciones mensuales asociadas a dichas fechas.\n",
    "\n",
    "Haciendo un análisis preliminar del archivo, observamos que existen datos de cuatro moléculas distintas (SO2, O3, NO y NO2) tomados desde 13 estaciones. Por lo tanto, tendremos que agrupar por *Molecule* y calcular la media de concentraciones en las distintas estaciones. Finalmente, como estos datos serán analizados como series temporales, crearemos un diccionario con la siguiente estructura:\n",
    "\n",
    "```\n",
    "{\n",
    "\"Molecule_1\": {\"registration date\": [31/01/18, 28/02/18,...], \"monthly concentration (ug/m3)\": [0, 0,...]},\n",
    "\"Molecule_2\": {\"registration date\": [31/01/18, 28/02/18,...], \"monthly concentration (ug/m3)\": [0, 0,...]},\n",
    "\"Molecule_3\": {\"registration date\": [31/01/18, 28/02/18,...], \"monthly concentration (ug/m3)\": [0, 0,...]},\n",
    "\"Molecule_4\": {\"registration date\": [31/01/18, 28/02/18,...], \"monthly concentration (ug/m3)\": [0, 0,...]},\n",
    "}\n",
    "```\n",
    "Donde \"Molecule_N\" debe ser sustituido por el nombre de la molécula. Es importante tener en cuenta que las fechas en el input de la función son strings separados por guiones, pero en el diccionario serán variables datetime separadas por barras.\n",
    "\n",
    "**Nota:** hay que tener en cuenta que las fechas están en formato dd/mm/yyyy.\n",
    "\n",
    "Una vez construido el diccionario, lo guardaremos en un pickle que llamaremos `pollution_data.pkl`.  Se incluye, además, una celda extra con el código que debéis utilizar para comprobar que la función se ejecuta correctamente.\n",
    "\n",
    "\n",
    "<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">SM</span> **(2 puntos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(path: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Create dictionary permite crear un diccionario con el formato con el\n",
    "    nombre de la molécula, registro de los datos y sus concentraciones.\n",
    "\n",
    "    Argumentos:\n",
    "    path : str\n",
    "        Ruta del archivo csv que contiene las mediciones de concentraciones.\n",
    "    date : str\n",
    "        Fecha de registro de las mediciones que deseamos obtener.\n",
    "\n",
    "    Retorno:\n",
    "    dict\n",
    "        Diccionario con el formato\n",
    "        {Molecule_n\": {\"registration date\": [31/01/18, ...],\n",
    "                        \"monthly concentration (ug/m3)\": [0,...]}}\n",
    "    \"\"\"\n",
    "\n",
    "    def filter_mol(molecule: str) -> pd.DataFrame:\n",
    "        \"\"\"filtramos los datos si el nombre de la molécula coincide con la\n",
    "        # molécula que deseamos\"\"\"\n",
    "        return df[df[\"Molecule\"] == molecule]\n",
    "\n",
    "    def mean_molecule(df_mol: pd.DataFrame) -> list:\n",
    "        \"\"\"Permite obtener la media de las concentraciones de la molécula\n",
    "        de cada fecha(columna) del dataframe filtrado\"\"\"\n",
    "        mean_mol = df_mol[2:].mean().values.tolist()\n",
    "        return [round(x, 2) for x in mean_mol]\n",
    "\n",
    "    # pasamos la fecha de formato str a formadato datetime\n",
    "    date_dt = datetime.datetime.strptime(date, \"%d-%m-%Y\")\n",
    "    # con panda leemos el archivo csv\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    # procedemos a poner en formato datatime aquellas columnas que\n",
    "    # contengan la fecha de registro de las mediciones\n",
    "    for Date in data.columns[2:]:\n",
    "        data.rename(columns={Date: datetime.datetime.strptime(\n",
    "            Date, \"%d/%m/%Y\")}, inplace=True)\n",
    "\n",
    "    # comparamos todas las columnas que correspondan a fechas anteriores\n",
    "    # o iguales a la fecha a la cual queremos obtener las concentraciones\n",
    "    filter_col = list(filter(lambda x: x <= date_dt, data.columns[2:]))\n",
    "\n",
    "    # seleccionamos las columnas de Station, Molecule, y a las que corresponden\n",
    "    # a las fechas filtradas\n",
    "    df = data[[\"Station\", \"Molecule\"] + filter_col]\n",
    "\n",
    "    # para guardar en el diccionario las fechas en el formato deseado\n",
    "    # procedemos a pasar de datetime a str, y la guardamos como lista\n",
    "    str_dates = [datetime.datetime.strftime(\n",
    "                x, \"%d/%m/%Y\") for x in df.columns[2:]]\n",
    "\n",
    "    # definimos el diccionario en el formato deseado, además hacemos\n",
    "    # el llamda a las funciones filter_mol y mean_molecule para obtener\n",
    "    # las concentraciones de cada molécula y su media\n",
    "    dict = {\"SO2\": {\"registration date\": str_dates,\n",
    "                    \"monthly concentration (ug/m3)\":\n",
    "                    mean_molecule(filter_mol(\"SO2\"))},\n",
    "\n",
    "            \"O3\": {\"registration date\": str_dates,\n",
    "                        \"monthly concentration (ug/m3)\":\n",
    "                        mean_molecule(filter_mol(\"O3\"))},\n",
    "\n",
    "            \"NO\": {\"registration date\": str_dates,\n",
    "                        \"monthly concentration (ug/m3)\":\n",
    "                        mean_molecule(filter_mol(\"NO\"))},\n",
    "\n",
    "            \"NO2\": {\"registration date\": str_dates,\n",
    "                    \"monthly concentration (ug/m3)\":\n",
    "                    mean_molecule(filter_mol(\"NO2\"))\n",
    "                    }}\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path: str, date: str):\n",
    "    with open(\"data/pollution_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(create_dictionary(path, date), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The monthly concentration of SO2 is {'registration date': ['31/01/2018', '28/02/2018', '31/03/2018', '30/04/2018', '31/05/2018', '30/06/2018', '31/07/2018', '31/08/2018', '30/09/2018', '31/10/2018', '30/11/2018', '31/12/2018', '31/01/2019', '28/02/2019', '31/03/2019', '30/04/2019', '31/05/2019', '30/06/2019', '31/07/2019', '31/08/2019', '30/09/2019', '31/10/2019', '30/11/2019', '31/12/2019', '31/01/2020', '29/02/2020', '31/03/2020', '30/04/2020', '31/05/2020', '30/06/2020', '31/07/2020', '31/08/2020', '30/09/2020', '31/10/2020', '30/11/2020', '31/12/2020'], 'monthly concentration (ug/m3)': [0.45, 0.47, 0.43, 0.46, 0.39, 0.44, 0.53, 0.51, 0.47, 0.46, 0.45, 0.5, 0.56, 0.63, 0.54, 0.46, 0.58, 0.72, 0.68, 0.6, 0.55, 0.57, 0.57, 0.6, 0.62, 0.62, 0.53, 0.51, 0.58, 0.63, 0.68, 0.66, 0.6, 0.59, 0.57, 0.57]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_8160\\1409804377.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  mean_mol = df_mol[2:].mean().values.tolist()\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "save_pickle(\"data/Pollution.csv\", \"31-12-2020\")\n",
    "pollution = pickle.load(open(\"data/pollution_data.pkl\", \"rb\"))\n",
    "print(\"The monthly concentration of SO2 is {}\".format(pollution[\"SO2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4\n",
    "\n",
    "Tras consolidar su posición en el mercado europeo, un empresa de transporte quiere expandir su negocio a Estados Unidos. Para ello nos ha encargado que definamos una o varias funciones para crear una estructura ordenada de ficheros a partir del fichero de entrada con los códigos postales de cada ciudad.\n",
    "\n",
    "El fichero de entrada que utilizaremos es `US_zipcodes.txt`, que está ubicado en la carpeta `data` y ha sido extraído de https://github.com/midwire/free_zipcode_data\n",
    "\n",
    "Este fichero no tiene encabezado y contiene un número indeterminado de líneas con el siguiente formato:\n",
    "```\n",
    "state, city, zipcode\n",
    "```\n",
    "\n",
    "donde\n",
    "- `state` es un código de dos caracteres que indica el estado.\n",
    "- `city` es el nombre de la ciudad.\n",
    "- `zipcode` es el código postal.\n",
    "\n",
    "La salida esperada es una estructura de directorios y ficheros como la descrita a continuación:\n",
    "\n",
    "```\n",
    "<US>\n",
    "    <state1>\n",
    "        <city1>\n",
    "            zipcode.txt\n",
    "        <city2>\n",
    "            zipcode.txt            \n",
    "        ...       \n",
    "    <state2>  \n",
    "    ...\n",
    "```\n",
    "\n",
    "Dentro de cada fichero `zipcode.txt` agruparemos los datos del fichero original por ciudad (cada ciudad puede contener varios códigos postales) siguiendo el siguiente formato:\n",
    "```\n",
    "Number of zipcodes: number_zipcodes\n",
    "Zipcode #1: zipcode\n",
    "Zipcode #2: zipcode\n",
    "...\n",
    "Zipcode #N: zipcode\n",
    "```\n",
    "\n",
    "Donde `number_zipcodes` es el número de códigos postales distintos que tiene la ciudad y `zipcode` es el código postal. El directorio a partir del cual se va a generar dicha estructura deberá ser un parámetro de la función.\n",
    "\n",
    "Una vez creada la estructura, necesitamos otra función que muestre por pantalla la ruta de las carpetas correspondientes a cada estado y el número de subcarpetas (ciudades) que contiene cada estado de la siguiente manera:\n",
    "```\n",
    "data/US/NV\n",
    "100\n",
    "data/US/KY\n",
    "843\n",
    "data/US/IN\n",
    "745\n",
    "data/US/NY\n",
    "1618\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">SM</span> **(3 puntos)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuesta 4\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_directory_file(input_file: str) -> list:\n",
    "    \"\"\"permite leer el archivo de directorio y devolver una lista con los\n",
    "    nombres de los archivos que contienen los archivos de datos de\n",
    "    concentraciones\"\"\"\n",
    "    with open(input_file, \"r\") as postal_file:\n",
    "        return [float(re.sub(\"\\n\", \"\", line).split(\": \")[1])\n",
    "                for line in postal_file]\n",
    "\n",
    "\n",
    "def write_directory_file(target_directories, directories):\n",
    "    \"\"\"permite escribir en el archivo de directorio aquella como el\n",
    "    total de los codigos ´postales y se enumera los mismos\"\"\"\n",
    "    with open(target_directories, \"w\") as directories_file:\n",
    "        directories_file.write(f\"Number of zipcodes: {directories[0]}\\n\")\n",
    "\n",
    "        for index, zipcode in enumerate(directories[1:]):\n",
    "            directories_file.write(f\"Zipcode # {index + 1}: {zipcode}\\n\")\n",
    "\n",
    "\n",
    "def create_file_system(input_file, target_directory):\n",
    "    \"\"\"\n",
    "    permite cear la estructura de directorios y archivos txt\n",
    "\n",
    "    Argumentos:\n",
    "    input_file : str\n",
    "        Ruta que contiene el acrhivo txt\n",
    "    target_directory : str\n",
    "        Ruta donde se crearán los directorios y archivos\n",
    "\n",
    "    Retorno:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # leemos el archivo de directorio\n",
    "    with open(input_file, \"r\") as postal_codes:\n",
    "        # recorremos cada línea del archivo\n",
    "        for line in postal_codes:\n",
    "            # guardamos en las variables aquellos datos separados por (,)\n",
    "            state, city, zipcode = line.split(\",\")\n",
    "\n",
    "            # creamos el directorio donde se crearán los archivos, ,ediante\n",
    "            # la ruta del directorio y el nombre de las variables\n",
    "            folder_target = os.path.join(target_directory, state, city)\n",
    "            os.makedirs(folder_target, exist_ok=True)\n",
    "\n",
    "            # especificamos la ruta en donde se va a crear el archivo txt\n",
    "            file_target = os.path.join(folder_target, \"zipcode.txt\")\n",
    "\n",
    "            # escribimos en el archivo txt donde si poseen la misma una\n",
    "            #  ciudad posee varios codigos postales este se guarda en una lista\n",
    "            # misma que se contará sus elementos para obtener el total de\n",
    "            # codigos postales, cas contrario existe solo 1 y este es el total\n",
    "            if os.path.exists(file_target):\n",
    "                zipcodes = read_directory_file(file_target)\n",
    "\n",
    "                zipcodes.append(int(zipcode))\n",
    "                zipcodes[0] = len(zipcodes) - 1\n",
    "\n",
    "                write_directory_file(file_target, zipcodes)\n",
    "\n",
    "            else:\n",
    "                write_directory_file(file_target, [\n",
    "                    1, int(zipcode)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_directory_contents(target_directory):\n",
    "    \"\"\"Permite imprimir el contenido de los directorios y archivos, además\n",
    "    de el número de carpetas que contiene.\"\"\"\n",
    "\n",
    "    for folder in os.listdir(target_directory):\n",
    "        print(f\"{target_directory}/{folder}\")\n",
    "        print(len(os.listdir(os.path.join(target_directory, folder))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/US/AK\n",
      "231\n",
      "data/US/AL\n",
      "587\n",
      "data/US/AR\n",
      "618\n",
      "data/US/AZ\n",
      "274\n",
      "data/US/CA\n",
      "1240\n",
      "data/US/CO\n",
      "404\n",
      "data/US/CT\n",
      "272\n",
      "data/US/DC\n",
      "4\n",
      "data/US/DE\n",
      "57\n",
      "data/US/FL\n",
      "524\n",
      "data/US/GA\n",
      "637\n",
      "data/US/HI\n",
      "94\n",
      "data/US/IA\n",
      "947\n",
      "data/US/ID\n",
      "270\n",
      "data/US/IL\n",
      "1307\n",
      "data/US/IN\n",
      "745\n",
      "data/US/KS\n",
      "633\n",
      "data/US/KY\n",
      "843\n",
      "data/US/LA\n",
      "492\n",
      "data/US/MA\n",
      "510\n",
      "data/US/MD\n",
      "440\n",
      "data/US/ME\n",
      "478\n",
      "data/US/MI\n",
      "890\n",
      "data/US/MN\n",
      "814\n",
      "data/US/MO\n",
      "963\n",
      "data/US/MS\n",
      "444\n",
      "data/US/MT\n",
      "365\n",
      "data/US/NC\n",
      "769\n",
      "data/US/ND\n",
      "383\n",
      "data/US/NE\n",
      "537\n",
      "data/US/NH\n",
      "254\n",
      "data/US/NJ\n",
      "580\n",
      "data/US/NM\n",
      "350\n",
      "data/US/NV\n",
      "100\n",
      "data/US/NY\n",
      "1618\n",
      "data/US/OH\n",
      "1072\n",
      "data/US/OK\n",
      "588\n",
      "data/US/OR\n",
      "384\n",
      "data/US/PA\n",
      "1825\n",
      "data/US/RI\n",
      "70\n",
      "data/US/SC\n",
      "380\n",
      "data/US/SD\n",
      "386\n",
      "data/US/TN\n",
      "556\n",
      "data/US/TX\n",
      "1486\n",
      "data/US/UT\n",
      "252\n",
      "data/US/VA\n",
      "853\n",
      "data/US/VT\n",
      "288\n",
      "data/US/WA\n",
      "496\n",
      "data/US/WI\n",
      "761\n",
      "data/US/WV\n",
      "826\n",
      "data/US/WY\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_file = \"data/US_zipcodes.txt\"\n",
    "target_directory = \"data/US\"\n",
    "create_file_system(input_file, target_directory)\n",
    "print_directory_contents(target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5\n",
    "\n",
    "Dada la estructura de ficheros del ejercicio anterior, queremos crear una función que comprima **el directorio raíz y todo su contenido** en un archivo con formato `tar.gz`.\n",
    "La función recibirá como *inputs* la ruta que contiene el fichero a comprimir y el directorio de destino donde se guardará el archivo comprimido.\n",
    "\n",
    "Además, queremos hacer un estudio del porcentaje de compresión en `tar.gz`. Para ello, definiremos una función que recibirá como *input* la ruta de un directorio o fichero (la propia función detectará si se trata de un directorio o de un fichero) y devolverá el tamaño total del contenido del directorio o del fichero, respectivamente.\n",
    "\n",
    "Una vez conocidos los tamaños de un directorio y del mismo comprimido en un fichero `tar.gz`, calcularemos el porcentaje de compresión.\n",
    "\n",
    "**Nota:** Podéis utilizar la librería *tarfile* de Python.\n",
    "\n",
    "<span style=\"font-family: Courier New; background-color: #ffcc5c; color: #000000; padding: 3px; \">EG</span> **(2 puntos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuesta 5.1\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "\n",
    "def tgz_folder(tgz_path, directory):\n",
    "    \"\"\"Permite comprimir un directorio en un archivo .tgz\"\"\"\n",
    "    # creamos el archivo .tgz mediante la libreria tarfile\n",
    "    with tarfile.open(f\"{tgz_path}.tar.gz\", \"w:gz\") as tgz:\n",
    "        tgz.add(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "directory = \"data/US\"\n",
    "tgz_path = \"data/US\"\n",
    "\n",
    "tgz_folder(tgz_path, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuesta 5.2\n",
    "\n",
    "def get_size(file_path: str) -> int:\n",
    "    \"\"\"Permite obtener el tamaño de un archivo de la ruta especificada\"\"\"\n",
    "    # hacemos uso de la libreria os para obtener el tamaño del archivo\n",
    "    # y su función getsize()\n",
    "    return os.path.getsize(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 8192, Tgz size: 1717964, Compression ratio: -208.71240234375\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "tgz_size = get_size(r\"data/US.tar.gz\")\n",
    "original_size = get_size(r\"data/US\")\n",
    "\n",
    "print(\n",
    "    f\"Original size: {original_size}, \"\n",
    "    f\"Tgz size: {tgz_size}, \"\n",
    "    f\"Compression ratio: {1 - tgz_size/original_size}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
